#!/usr/bin/env python

import os
import sys
import uuid
from threading import Thread
from typing import Dict, Any
import argparse
from flask import Flask, request, jsonify
from flask_cors import CORS
import tensorflow as tf
from tensorflow.python.platform import gfile

import tensorflow_encrypted as tfe
from tensorflow_encrypted.convert import convert
from tensorflow_encrypted.convert.register import register

_global_memory: Dict[str, Any] = {}

# define the app
app = Flask(__name__)
CORS(app)  # needed for cross-domain requests, allow everything by default

node_name = os.environ.get('TFE_NODE_NAME')
server0_address = os.environ.get('TFE_SERVER0_ADDRESS')
server1_address = os.environ.get('TFE_SERVER1_ADDRESS')
producer_address = os.environ.get('TFE_CRYPTO_PRODUCER_ADDRESS')
weight_address = os.environ.get('TFE_WEIGHT_PROVIDER_ADDRESS')

if node_name is None:
    print("The name of this node must be provided via the 'TFE_NODE_NAME' environment variable")
    sys.exit(1)

if server0_address is None:
    print("The address for server 0 must be provided via the 'TFE_SERVER0_ADDRESS' environment variable")
    sys.exit(1)

if server1_address is None:
    print("The address for server 1 must be provided via the 'TFE_SERVER1_ADDRESS' environment variable")
    sys.exit(1)

if producer_address is None:
    print("The address for the crypto producer must be provided via the 'TFE_CRYPTO_PRODUCER_ADRESS' environment variable")
    sys.exit(1)

if weight_address is None:
    print("The address for the weight provider must be provided via the 'TFE_WEIGHT_PROVIDER_ADDRESS' environment variable")
    sys.exit(1)

config = tfe.RemoteConfig({
        'server0': server0_address,
        'server1': server1_address,
        'crypto_producer': producer_address,
        'weights_provider': weight_address,
    },
    master_host=node_name
)


def get_model_predict(config):

    print('Serving model: %s' % config.model_name)
    model_file = os.path.join(
        os.path.dirname(os.path.realpath(__file__)),
        "..",
        "models",
        "%s.pb" % config.model_name,
    )

    tf.reset_default_graph()

    class PredictionInputProvider(tfe.io.InputProvider):
        def provide_input(self) -> tf.Tensor:
            return tf.placeholder(tf.float32, shape=[1, 16], name="api")

    class PredictionOutputReceiver(tfe.io.OutputReceiver):
        def receive_output(self, tensor: tf.Tensor) -> tf.Tensor:
            return tf.Print(tensor, [tensor], message="tf.Print(output): ")

    with gfile.FastGFile(model_file, 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())

    sess = config.session()
    with tfe.protocol.Pond(*config.get_players('server0, server1, crypto_producer')) as prot:
        # The machine which feeds input must be the flask server for now
        input = PredictionInputProvider(config.get_player('server0'))

        c = convert.Converter(config, prot, config.get_player('weights_provider'))
        x = c.convert(graph_def, input, register())

        # Not sure how to use this scheme for now
        # output = PredictionOutputReceiver(config.get_player('master'))
        # prediction_op = prot.define_output(x, output)

        tfe.run(sess, prot.initializer, tag='init')

    pl = tf.get_default_graph().get_tensor_by_name("private-input/api:0")

    def predict_fn(request_id: str, input_data: Any) -> None:
        output_data = x.reveal().eval(
            sess=sess,
            feed_dict={pl: input_data},
            tag='prediction',
        )

        _global_memory[request_id] = output_data[0].tolist()

    return predict_fn


@app.route('/poll', methods=['POST'])
def api_poll():
    request_id = request.json
    app.logger.info("api_poll_input: " + str(request_id))
    print(_global_memory)

    output_data = _global_memory.pop(request_id, None)

    app.logger.info("api_poll_output: " + str(output_data))
    response = jsonify(output_data)
    return response


@app.route('/')
def index():
    return "Welcome to PartyDB! An crypto-friendly production environment for serving machine learning models"


# HTTP Errors handlers
@app.errorhandler(404)
def url_error(e):
    return """
    Wrong URL!
    <pre>{}</pre>""".format(e), 404


@app.errorhandler(500)
def server_error(e):
    return """
    An internal error occurred: <pre>{}</pre>
    See logs for full stacktrace.
    """.format(e), 500


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--model_name", type=str, default="matmul", help="increase output verbosity")
    args = parser.parse_args()

    predict = get_model_predict(args)

    # API route
    @app.route('/predict', methods=['POST'])
    def api_predict():
        input_data = request.json
        app.logger.info("api_predict_input: " + str(input_data))

        request_id = str(uuid.uuid4())

        thread = Thread(target=predict, kwargs={
            'request_id': request_id,
            'input_data': input_data
        })
        thread.start()

        app.logger.info("api_predict_output: " + request_id)
        data = {'request_id': request_id}
        response = jsonify(data)
        return response

    app.run(host='0.0.0.0', debug=True)
