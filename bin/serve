#!/usr/bin/env python
"""Spin up a TFE cluster for handling serving requests."""

import sys
import os
import time
from typing import Callable, List
from collections import OrderedDict

import tensorflow as tf
from tensorflow.python.platform import gfile
import numpy as np

import tf_encrypted as tfe
from tf_encrypted.protocol import Pond, SecureNN

node_name = os.environ.get('TFE_NODE_NAME')
server0_address = os.environ.get('TFE_SERVER0_ADDRESS')
server1_address = os.environ.get('TFE_SERVER1_ADDRESS')
producer_address = os.environ.get('TFE_CRYPTO_PRODUCER_ADDRESS')
provider_address = os.environ.get('TFE_WEIGHTS_PROVIDER_ADDRESS')
master_address = os.environ.get('TFE_MASTER_ADDRESS')
number_of_reqs = os.environ.get('TFE_NUMBER_REQUESTS')
port = int(os.environ.get('TFE_API_PORT', "8080"))
batch_size = int(os.environ.get('TFE_BATCH_SIZE', 1))
protocol_name = str(os.environ.get('TFE_PROTOCOL_NAME', 'securenn'))
enable_events_monitor = os.environ.get('TFE_MONITOR_EVENTS') is not None
enable_traces = os.environ.get('TFE_TRACE') is not None

env_truncation = os.environ.get('TFE_TRUNCATION_TYPE') or ""
interactive_truncation = True
if str.upper(env_truncation) == 'NON_INTERACTIVE':
  interactive_truncation = False


def fixed_config():
  """Determines the fixed-point configuraiton to use."""
  if tensorflow_supports_int64():
    if interactive_truncation:
      print('using fixed64 interactive truncation')
      return fixed.fixed64
    print('using fixed64 non interactive truncation')
    return fixed.fixed64_ni
  if interactive_truncation:
    print('using fixed100 interactive truncation')
    return fixed.fixed100
  print('using fixed100 non interactive truncation')
  return fixed.fixed100_ni


if node_name is None:
  err = ("The node_name of this node must be provided via"
         "the 'TFE_NODE_NAME' environment variable")
  print(err)
  sys.exit(1)

if server0_address is None:
  err = ("The address for server 0 must be provided via"
         "the 'TFE_SERVER0_ADDRESS' environment variable")
  print(err)
  sys.exit(1)

if server1_address is None:
  err = ("The address for server 1 must be provided via"
         "the 'TFE_SERVER1_ADDRESS' environment variable")
  print(err)
  sys.exit(1)

if producer_address is None:
  err = ("The address for the producer must be provided via"
         "the 'TFE_CRYPTO_PRODUCER_ADRESS' environment variable")
  print(err)
  sys.exit(1)

if provider_address is None:
  err = ("The address for the weights provider must be provided via"
         "the 'TFE_WEIGHTS_PROVIDER_ADDRESS' environment variable")
  print(err)
  sys.exit(1)

if master_address is None:
  err = ("The address for master must be provided via"
         "the 'TFE_MASTER_ADDRESS' environment variable")
  print(err)
  sys.exit(1)

tfe.set_tfe_events_flag(enable_events_monitor)
tfe.set_tfe_trace_flag(enable_traces)

remote_config = tfe.RemoteConfig(
    OrderedDict([
        ('master', master_address),
        ('server0', server0_address),
        ('server1', server1_address),
        ('crypto-producer', producer_address),
        ('weights-provider', provider_address)
    ])
)

tfe.set_config(remote_config)

if protocol_name == 'securenn':
  tfe.set_protocol(SecureNN())
else:
  tfe.set_protocol(Pond())

print('Using protocol: {}'.format(protocol_name))
server = remote_config.server(node_name)
if node_name != 'master':
  server.join()
else:

  _global_memory = {}

  tf.reset_default_graph()

  input_spec = []
  model_file = 'model.pb'
  with gfile.GFile(model_file, 'rb') as f:
    print('Loading model: {}'.format(model_file))
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())

    for node in graph_def.node:
      if node.op != "Placeholder":
        continue

      not_batch = [int(d.size) for d in node.attr['shape'].shape.dim[1:]]
      input_shape = [batch_size] + not_batch

      input_spec.append({
          'name': node.name,
          'dtype': node.attr['dtype'].type,
          'shape': input_shape
      })

  sess = tfe.Session()

  pls = []
  inputs = []
  for i, spec in enumerate(input_spec):
    def scope(provider_index, spec_in: dict) -> Callable[[], tf.Tensor]:
      def provide_input() -> tf.Tensor:
        pl = tf.placeholder(
            tf.float32,
            shape=spec_in['shape'],
            name="api/{}".format(provider_index))
        pls.append(pl)
        return pl

      return provide_input

    inputs.append(scope(i, spec))

  def predict(input_data: List[np.ndarray]) -> None:
    feed_dict = {pl: input_data[i] for i, pl in enumerate(pls)}
    output_data = sess.run(y, feed_dict=feed_dict, tag='prediction')
    return output_data

  print('Securing computation')
  secure_start = time.time()
  c = tfe.convert.convert.Converter(
      tfe.convert.registry(),
      config=remote_config,
      protocol=tfe.get_protocol(),
      model_provider=remote_config.get_player('weights-provider'),
  )
  y = c.convert(graph_def, remote_config.get_player('master'), inputs)
  # TODO: do not reveal on the master server
  y = y.reveal()
  secure_end = time.time()
  print("Graph secured in {}".format(secure_end - secure_start))

  print('Initializing variables')
  init_start = time.time()
  sess.run(tfe.global_variables_initializer(), tag='init')
  init_end = time.time()
  print("Graph initialized in {}".format(init_end - init_start))

  print("Warming up the graph")
  warmup_data = [np.random.standard_normal(
      spec['shape']).tolist() for spec in input_spec]
  warmup_start = time.time()
  predict(warmup_data)
  warmup_end = time.time()
  print("Graph warmed up in {}".format(warmup_end - warmup_start))
  print('Serving model')

  print('running the graph')

  run_data = [np.random.standard_normal(
      spec['shape']).tolist() for spec in input_spec]
  runs = []
  for x in range(0, int(number_of_reqs)):
    predict(run_data)
    print('finished epoch {} of {}'.format(x + 1, number_of_reqs))
