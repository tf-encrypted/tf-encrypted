{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public Training with Differential Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By training your model with differential privacy, you can ensure that the model is not memorizing sensitive informations about the training set. If the model is not trained with differential privacy, attackers could reveal some private imformations by just querring the deployed model. Two common attacks are [membership inference](https://www.cs.cornell.edu/~shmat/shmat_oak17.pdf) and [model inversion](https://www.cs.cmu.edu/~mfredrik/papers/fjr2015ccs.pdf).\n",
    "\n",
    "To learn mode about [TensorFlow Privacy](https://github.com/tensorflow/privacy) you can read this [excellent blog post](http://www.cleverhans.io/privacy/2019/03/26/machine-learning-with-differential-privacy-in-tensorflow.html). Before running this notebook, please install [TensorFlow Privacy](https://github.com/tensorflow/privacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training a CNN on MNIST with Keras and the DP SGD optimizer.\n",
    "source: https://github.com/tensorflow/privacy/blob/master/tutorials/mnist_dpsgd_tutorial_keras.py\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "from distutils.version import LooseVersion\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from privacy.analysis.rdp_accountant import compute_rdp\n",
    "from privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "from privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
    "\n",
    "import sys\n",
    "sys.argv = sys.argv[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LooseVersion(tf.__version__) < LooseVersion('2.0.0'):\n",
    "    GradientDescentOptimizer = tf.train.GradientDescentOptimizer\n",
    "else:\n",
    "    GradientDescentOptimizer = tf.optimizers.SGD  # pylint: disable=invalid-name\n",
    "\n",
    "flags.DEFINE_boolean(\n",
    "    'dpsgd', True, 'If True, train with DP-SGD. If False, '\n",
    "    'train with vanilla SGD.')\n",
    "flags.DEFINE_float('learning_rate', 0.15, 'Learning rate for training')\n",
    "flags.DEFINE_float('noise_multiplier', 1.1,\n",
    "                   'Ratio of the standard deviation to the clipping norm')\n",
    "flags.DEFINE_float('l2_norm_clip', 1.0, 'Clipping norm')\n",
    "flags.DEFINE_integer('batch_size', 250, 'Batch size')\n",
    "flags.DEFINE_integer('epochs', 20, 'Number of epochs')\n",
    "flags.DEFINE_integer(\n",
    "    'microbatches', 50, 'Number of microbatches '\n",
    "    '(must evenly divide batch_size)')\n",
    "flags.DEFINE_string('model_dir', None, 'Model directory')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "_ = FLAGS(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0809 15:18:04.871719 4633503168 deprecation.py:323] From /anaconda3/envs/pysyft_36/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0809 15:18:04.898653 4633503168 deprecation_wrapper.py:119] From /Users/yanndupis/Documents/dropoutlabs/privacy/privacy/dp_query/gaussian_query.py:101: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 96s 2ms/sample - loss: 1.3357 - acc: 0.5958 - val_loss: 0.8256 - val_acc: 0.7215\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 107s 2ms/sample - loss: 0.7292 - acc: 0.7667 - val_loss: 0.6640 - val_acc: 0.7864\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 113s 2ms/sample - loss: 0.6328 - acc: 0.8052 - val_loss: 0.6256 - val_acc: 0.8109\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 110s 2ms/sample - loss: 0.5508 - acc: 0.8370 - val_loss: 0.4953 - val_acc: 0.8608\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 114s 2ms/sample - loss: 0.5059 - acc: 0.8565 - val_loss: 0.4566 - val_acc: 0.8737\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 124s 2ms/sample - loss: 0.4820 - acc: 0.8660 - val_loss: 0.4214 - val_acc: 0.8836\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 111s 2ms/sample - loss: 0.4549 - acc: 0.8794 - val_loss: 0.4024 - val_acc: 0.8946\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 127s 2ms/sample - loss: 0.4293 - acc: 0.8892 - val_loss: 0.3751 - val_acc: 0.9032\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 141s 2ms/sample - loss: 0.3985 - acc: 0.8987 - val_loss: 0.3493 - val_acc: 0.9074\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 134s 2ms/sample - loss: 0.3752 - acc: 0.9041 - val_loss: 0.3377 - val_acc: 0.9105\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 139s 2ms/sample - loss: 0.3578 - acc: 0.9118 - val_loss: 0.3448 - val_acc: 0.9152\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 142s 2ms/sample - loss: 0.3518 - acc: 0.9148 - val_loss: 0.3085 - val_acc: 0.9219\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 139s 2ms/sample - loss: 0.3524 - acc: 0.9183 - val_loss: 0.3267 - val_acc: 0.9197\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 120s 2ms/sample - loss: 0.3449 - acc: 0.9215 - val_loss: 0.3100 - val_acc: 0.9247\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 127s 2ms/sample - loss: 0.3425 - acc: 0.9248 - val_loss: 0.2800 - val_acc: 0.9342\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 137s 2ms/sample - loss: 0.3448 - acc: 0.9255 - val_loss: 0.3110 - val_acc: 0.9329\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 143s 2ms/sample - loss: 0.3469 - acc: 0.9264 - val_loss: 0.3422 - val_acc: 0.9305\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 127s 2ms/sample - loss: 0.3580 - acc: 0.9271 - val_loss: 0.3163 - val_acc: 0.9352\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 127s 2ms/sample - loss: 0.3554 - acc: 0.9281 - val_loss: 0.3260 - val_acc: 0.9354\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 129s 2ms/sample - loss: 0.3594 - acc: 0.9295 - val_loss: 0.3364 - val_acc: 0.9340\n",
      "For delta=1e-5, the current epsilon is: 1.76\n"
     ]
    }
   ],
   "source": [
    "def compute_epsilon(steps):\n",
    "    \"\"\"Computes epsilon value for given hyperparameters.\"\"\"\n",
    "    if FLAGS.noise_multiplier == 0.0:\n",
    "        return float('inf')\n",
    "    orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n",
    "    sampling_probability = FLAGS.batch_size / 60000\n",
    "    rdp = compute_rdp(q=sampling_probability,\n",
    "                    noise_multiplier=FLAGS.noise_multiplier,\n",
    "                    steps=steps,\n",
    "                    orders=orders)\n",
    "    # Delta is set to 1e-5 because MNIST has 60000 training points.\n",
    "    return get_privacy_spent(orders, rdp, target_delta=1e-5)[0]\n",
    "\n",
    "\n",
    "def load_mnist():\n",
    "    \"\"\"Loads MNIST and preprocesses to combine training and validation data.\"\"\"\n",
    "    train, test = tf.keras.datasets.mnist.load_data()\n",
    "    train_data, train_labels = train\n",
    "    test_data, test_labels = test\n",
    "\n",
    "    train_data = np.array(train_data, dtype=np.float32) / 255\n",
    "    test_data = np.array(test_data, dtype=np.float32) / 255\n",
    "\n",
    "    train_data = train_data.reshape(train_data.shape[0], 28, 28, 1)\n",
    "    test_data = test_data.reshape(test_data.shape[0], 28, 28, 1)\n",
    "\n",
    "    train_labels = np.array(train_labels, dtype=np.int32)\n",
    "    test_labels = np.array(test_labels, dtype=np.int32)\n",
    "    \n",
    "    train_labels = tf.keras.utils.to_categorical(train_labels, num_classes=10)\n",
    "    test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "    assert train_data.min() == 0.\n",
    "    assert train_data.max() == 1.\n",
    "    assert test_data.min() == 0.\n",
    "    assert test_data.max() == 1.\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "if FLAGS.dpsgd and FLAGS.batch_size % FLAGS.microbatches != 0:\n",
    "    raise ValueError('Number of microbatches should divide evenly batch_size')\n",
    "\n",
    "# Load training and test data.\n",
    "train_data, train_labels, test_data, test_labels = load_mnist()\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(16, 8,\n",
    "                             strides=2,\n",
    "                             padding='same',\n",
    "                             activation='relu',\n",
    "                             input_shape=(28, 28, 1)),\n",
    "      tf.keras.layers.AveragePooling2D(2, 1),\n",
    "      tf.keras.layers.Conv2D(32, 4,\n",
    "                             strides=2,\n",
    "                             padding='valid',\n",
    "                             activation='relu'),\n",
    "      tf.keras.layers.AveragePooling2D(2, 1),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(32, activation='relu'),\n",
    "      tf.keras.layers.Dense(10)\n",
    "  ])\n",
    "\n",
    "if FLAGS.dpsgd:\n",
    "    optimizer = DPGradientDescentGaussianOptimizer(\n",
    "        l2_norm_clip=FLAGS.l2_norm_clip,\n",
    "        noise_multiplier=FLAGS.noise_multiplier,\n",
    "        num_microbatches=FLAGS.microbatches,\n",
    "        learning_rate=FLAGS.learning_rate)\n",
    "    # Compute vector of per-example loss rather than its mean over a minibatch.\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.losses.Reduction.NONE)\n",
    "else:\n",
    "    optimizer = GradientDescentOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Compile model with Keras\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "# Train model with Keras\n",
    "model.fit(train_data, train_labels,\n",
    "        epochs=FLAGS.epochs,\n",
    "        validation_data=(test_data, test_labels),\n",
    "        batch_size=FLAGS.batch_size)\n",
    "\n",
    "# Compute the privacy budget expended.\n",
    "if FLAGS.dpsgd:\n",
    "    eps = compute_epsilon(FLAGS.epochs * 60000 // FLAGS.batch_size)\n",
    "    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
    "else:\n",
    "    print('Trained with vanilla non-private SGD optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0809 16:01:29.860492 4633503168 hdf5_format.py:110] TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file. You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
     ]
    }
   ],
   "source": [
    "model.save('short-dnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
