{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how runtime performance of the `tfe.protocol.pond` protocol can be improved by splitting the computation into an offline and an online phase. It assumes some understanding of how the protocol works at a cryptographic layer, at the very least an understanding of the notion of triples, and is intended for intermediate to advanced users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tf_encrypted as tfe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation\n",
    "\n",
    "As a first step in defining our computation we have to select which triple source we want to use for the Pond protocol. By default this is `OnlineTripleSource` but here we want to used the queued version instead. Below we leave the option to select either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triple_source = tfe.protocol.pond.OnlineTripleSource(\"server2\")\n",
    "# tfe.set_protocol(tfe.protocol.Pond(\"server0\", \"server1\", triple_source=triple_source))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triple_source = tfe.protocol.pond.QueuedOnlineTripleSource(\"server0\", \"server1\", \"server2\", capacity=10)\n",
    "tfe.set_protocol(tfe.protocol.Pond(\"server0\", \"server1\", triple_source=triple_source))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our actual computation we are interested in two values, `y` and `w`, as defined next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tfe.define_private_input(\"coefficients-provider\", lambda: tf.constant([1,2,3,4,5,6,7,8,9,10], shape=[10, 1]))\n",
    "\n",
    "x = tfe.define_private_input(\"data-provider\", lambda: tf.fill([1, 10], 1))\n",
    "y = tfe.matmul(x, c).reveal()\n",
    "\n",
    "v = tfe.define_private_input(\"data-provider\", lambda: tf.fill([1, 10], 2))\n",
    "w = tfe.matmul(v, c).reveal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Offline and Online Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_triple_status():\n",
    "\n",
    "    if not hasattr(triple_source, \"queues\"):\n",
    "        return\n",
    "\n",
    "    for queue in triple_source.queues:\n",
    "        print(\"{:40}: {:>2} / {:>2}\".format(\n",
    "            queue.name,\n",
    "            queue.size(),\n",
    "            triple_source.capacity)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the queued triple source the computation is split into two phases:\n",
    "\n",
    "- an *offline* phase where triples are generated and distributed by a trusted third-party\n",
    "\n",
    "- an *online* phase where the actual values are computed by consuming triples previously generated, and which does not involve the trusted third-party\n",
    "\n",
    "The most important thing it to always maintain a correspondance between these two phases, so that when we run an online computation we are consuming triples that where made to match for every node in the subgraph that defines it.\n",
    "\n",
    "To get started let us first update our cache. To generate triples for this we simply ask the triple source, passing in the fetch we intend to run later. This in itself does not run anything, it simply figures out how to generate triples for the particular fetch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_triple_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we then run the new fetch to actually generate triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_triple_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we run our online computation, consuming what we just generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_triple_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way of maintaining the correspondance between the offline and online phase is to alternate between them as we have just done, first running the offline computation and then immediately running the online computation.\n",
    "\n",
    "But we can also work with larger sequences as shown next, generating triples for several fetches before running any online computation. Be careful not to overdo this though, as TensorFlow will block when the queues fill up and exceeds their capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_triples = triple_source.generate_triples(y)\n",
    "w_triples = triple_source.generate_triples(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(y_triples, tag='y_triples')\n",
    "sess.run(w_triples, tag='w_triples')\n",
    "\n",
    "print_triple_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_y = sess.run(y, tag='y')\n",
    "res_w = sess.run(w, tag='w')\n",
    "print(res_y, res_w)\n",
    "\n",
    "print_triple_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of running a sequence of fetches as above, one might be tempted to use either `sess.run([y_triples, w_triples])` or `sess.run([y, w])`. This is fine as long as it does not introduce non-determinism into the computation, which could break the correspondance we have to maintain.\n",
    "\n",
    "For instance, if we had used `tfe.cache(c)` instead of `tfe.cache(tfe.mask(c))` then there is an overlap in the computation of `y` and `w` that can cause different evaluation orders to break the correspondance and give wrong results. To force this to happen you can modify the computation earlier and change the evaluation order of `y` and `w` as done below: offline is for sequence `[y, w]` but online is for `[w, y]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess.run(y_triples)\n",
    "# sess.run(w_triples)\n",
    "\n",
    "# res_w = sess.run(w)\n",
    "# res_y = sess.run(y)\n",
    "# print(res_y, res_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can safely streamline our process and generate triples for the next run while executing the current. The reason this works is because of the `tf.queue.FIFOQueue`s used by the triple source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(y_triples)\n",
    "\n",
    "res, _ = sess.run([y, y_triples], tag='streamlined')\n",
    "print(res)\n",
    "\n",
    "res, _ = sess.run([y, y_triples], tag='streamlined')\n",
    "print(res)\n",
    "\n",
    "res = sess.run(y)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tfe2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2b3cf1a821f25a2bafd97d4b4f3a77d0e1f42d8d78c0e512d3a72b703c70416e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
